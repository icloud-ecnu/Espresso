{
    "batch_size_per_gpu": 2,
    "max_batch_size_per_gpu": 2,
    "gradient_accumulation_steps": 2,
    "global_batch_size": 2560,
    "dp_size": 640,
    "tp_size": 1,
    "pp_size": 1,
    "sp_size": 1,
    "ds_zero": "STAGE_3",
    "total_num_gpus": 640,
    "seq_len": 2048,
    "total_num_tokens": 300000000000.0,
    "num_params_total": 529511874560,
    "activation_recomputation": "FULL",
    "achived_flops": 138.0,
    "flops_efficiency": 0.4423076923076923,
    "hbm_memory_efficiency": 1,
    "num_flops_total_per_micro_batch": 17639666907873280,
    "weight_memory_per_gpu": 3710648320.0,
    "gradient_memory_per_gpu": 1651507200.0,
    "optimizer_state_memory_per_gpu": 13212057600.0,
    "activation_memory_bs1": 8808038400.0,
    "activation_memory_per_gpu": 17616076800.0,
    "latency_per_micro_batch": 127.82367324545855,
    "latency_fwd": 32.00255718488232,
    "latency_fwd_attn": 10.98017726107826,
    "latency_fwd_mlp": 20.914623354434784,
    "latency_fwd_layernorm": 0.0453146670096463,
    "latency_fwd_tp_comm": 0,
    "latency_fwd_input_embedding": 0.0013242065080385852,
    "latency_fwd_output_embedding_loss": 0.061117695851594205,
    "latency_per_iter": 255.6473464909171,
    "total_training_latency": 14628141.166210277,
    "gpu_hours": 2600558.4295484936
}
