{
    "batch_size_per_gpu": 2,
    "max_batch_size_per_gpu": 7,
    "gradient_accumulation_steps": 1,
    "global_batch_size": 1536,
    "dp_size": 768,
    "tp_size": 1,
    "pp_size": 1,
    "sp_size": 1,
    "ds_zero": "STAGE_3",
    "total_num_gpus": 768,
    "seq_len": 2048,
    "total_num_tokens": 300000000000.0,
    "num_params_total": 174563917824,
    "activation_recomputation": "FULL",
    "achived_flops": 88.0,
    "flops_efficiency": 0.28205128205128205,
    "hbm_memory_efficiency": 1,
    "num_flops_total_per_micro_batch": 5878440133656576,
    "weight_memory_per_gpu": 1688469504.0,
    "gradient_memory_per_gpu": 452984832.0,
    "optimizer_state_memory_per_gpu": 3623878656.0,
    "activation_memory_bs1": 4831838208.0,
    "activation_memory_per_gpu": 9663676416.0,
    "latency_per_micro_batch": 66.80045606427927,
    "latency_fwd": 16.72576687159136,
    "latency_fwd_attn": 5.847402747717818,
    "latency_fwd_mlp": 10.795205072709818,
    "latency_fwd_layernorm": 0.024858331616720256,
    "latency_fwd_tp_comm": 0,
    "latency_fwd_input_embedding": 0.0007945239048231511,
    "latency_fwd_output_embedding_loss": 0.05750619564218182,
    "latency_per_iter": 66.80045606427927,
    "total_training_latency": 6370559.093482122,
    "gpu_hours": 1359052.6066095193
}
