
----------------------------------------------Summary-----------------------------------------------
batch_size_per_gpu: 8
max_batch_size_per_gpu: 9
gradient_accumulation_steps: 15625
global_batch_size: 4000000
dp_size: 32
tp_size: 8
pp_size: 8
sp_size: 8
ds_zero: NONE
total_num_gpus: 2048
seq_len: 2048
total_num_tokens: 1.4 T
num_params_total: 64.69 G
activation_recomputation: SELECTIVE
achived_flops: 312
flops_efficiency: 1
hbm_memory_efficiency: 1
num_flops_total_per_micro_batch: 6710.8 T
weight_memory_per_gpu: 2.08 GB
gradient_memory_per_gpu: 2.01 GB
optimizer_state_memory_per_gpu: 16.11 GB
activation_memory_bs1: 6.21 GB
activation_memory_per_gpu: 49.66 GB
latency_per_micro_batch: 336.08 ms
latency_fwd: 147.37 ms
latency_fwd_attn: 39.65 ms
latency_fwd_mlp: 70.48 ms
latency_fwd_layernorm: 658.25 us
latency_fwd_tp_comm: 31.32 ms
latency_fwd_input_embedding: 1.82 ms
latency_fwd_output_embedding_loss: 3.44 ms
latency_per_iter: 1.46 hours
total_training_latency: 10.33 days
gpu_hours: 507850
----------------------------------------------------------------------------------------------------
