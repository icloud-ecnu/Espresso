{
    "batch_size_per_gpu": 25,
    "max_batch_size_per_gpu": 39,
    "gradient_accumulation_steps": 5000,
    "global_batch_size": 4000000,
    "dp_size": 32,
    "tp_size": 8,
    "pp_size": 8,
    "sp_size": 8,
    "ds_zero": "NONE",
    "total_num_gpus": 2048,
    "seq_len": 2048,
    "total_num_tokens": 1000000000000.0,
    "num_params_total": 12746752000,
    "activation_recomputation": "SELECTIVE",
    "achived_flops": 312,
    "flops_efficiency": 1,
    "hbm_memory_efficiency": 1,
    "num_flops_total_per_micro_batch": 4259399598080000,
    "weight_memory_per_gpu": 434176000.0,
    "gradient_memory_per_gpu": 393216000.0,
    "optimizer_state_memory_per_gpu": 3145728000.0,
    "activation_memory_bs1": 1939865600.0,
    "activation_memory_per_gpu": 48496640000.0,
    "latency_per_micro_batch": 0.21331127794871796,
    "latency_fwd": 0.10999658960425548,
    "latency_fwd_attn": 0.02581110153846154,
    "latency_fwd_mlp": 0.04301850256410256,
    "latency_fwd_layernorm": 0.0006428249141736146,
    "latency_fwd_tp_comm": 0.030583466666666666,
    "latency_fwd_input_embedding": 0.00321905289521007,
    "latency_fwd_output_embedding_loss": 0.006721641025641025,
    "latency_per_iter": 1066.5563897435898,
    "total_training_latency": 130119.87954871794,
    "gpu_hours": 74023.75369882621
}
