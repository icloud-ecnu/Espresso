
----------------------------------------------Summary-----------------------------------------------
batch_size_per_gpu: 25
max_batch_size_per_gpu: 39
gradient_accumulation_steps: 5000
global_batch_size: 4000000
dp_size: 32
tp_size: 8
pp_size: 8
sp_size: 8
ds_zero: NONE
total_num_gpus: 2048
seq_len: 2048
total_num_tokens: 1.0 T
num_params_total: 12.75 G
activation_recomputation: SELECTIVE
achived_flops: 312
flops_efficiency: 1
hbm_memory_efficiency: 1
num_flops_total_per_micro_batch: 4259.4 T
weight_memory_per_gpu: 434.18 MB
gradient_memory_per_gpu: 393.22 MB
optimizer_state_memory_per_gpu: 3.15 GB
activation_memory_bs1: 1.94 GB
activation_memory_per_gpu: 48.5 GB
latency_per_micro_batch: 213.31 ms
latency_fwd: 110.0 ms
latency_fwd_attn: 25.81 ms
latency_fwd_mlp: 43.02 ms
latency_fwd_layernorm: 642.82 us
latency_fwd_tp_comm: 30.58 ms
latency_fwd_input_embedding: 3.22 ms
latency_fwd_output_embedding_loss: 6.72 ms
latency_per_iter: 17.78 minutes
total_training_latency: 1.51 days
gpu_hours: 74023
----------------------------------------------------------------------------------------------------
